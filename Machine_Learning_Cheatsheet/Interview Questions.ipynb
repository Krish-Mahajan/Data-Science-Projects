{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interview Questions  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What is Regularization? Which problem does Regularization try to solve?  \n",
    "**(soln)** Technique to solve overfitting problem in Machine Learning  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How you can fit a non-linear relations between X (say, Age) and Y (say, Income) into a Linear Model? - I also expect candidates to show me mathematically the marginal effect of X on Y based on their proposed solution.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Which Clustering methods you are familiar with?   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Time Series: if you have a data-set with 100 observations for each Xi, and 3 lag-effect variables of X1, how many predictions you will have if you will run any simple linear regression?   \n",
    "\n",
    " \n",
    "\n",
    "\n",
    "- **What is Box-Cox transformation?**   \n",
    "\n",
    "\n",
    "\n",
    "- **Will Gradient Descent methods always converge to the same point?**    \n",
    "**(soln)** No\n",
    "\n",
    "- **Clustering is also a common question in this case, I usually ask:**   \n",
    "How do you find the optimal k (k*) in K-Mean? (I expect candidates to know at least 2 methods)  \n",
    "\n",
    "\n",
    "\n",
    "- **Bootstrapping**  \n",
    "Resampling with replacement(same as Bagging)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **significance of the ROC curve and how to interpret a ROC curve ?**    \n",
    "**(soln)**    \n",
    "sensitivity= recall = True positive rate = TP/Actual yes (When it's actually yes,how often does it predict yes)  \n",
    "specificity = 1 - False Positive rate = TN/Actual No (When it's actually no, how often doest it predict no)  \n",
    "precision = \"When it predicts yes,How often it is correct\"\n",
    "F - score = \"It is weighted average of recall and precision\"  \n",
    "ROC curve = An ROC curve is the most commonly used way to visualize the performance of a binary classifier, and AUC is (arguably) the best way to summarize its performance in a single number .It is generated by plotting the True Positive Rate (y-axis) against the False Positive Rate (x-axis) as you vary the threshold for assigning observations to a given class "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When you have a time series data by monthly, it has large data records, how will you find out significant difference between this month and previous month     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **How to deal with unbalance data where the ratio of positive and negative is huge?**  \n",
    "    - Try collecting more data .(silly but very helpful)  \n",
    "    - Try changing performance matrix(confusion matrix,precision-recall,AUC/ROC,f-1 score)\n",
    "    - Resampling Dataset  \n",
    "        - Add pieces of instances from the under-represented class called over sampling  \n",
    "        - Delete pieces from the over represented class,called under-sampling  \n",
    "        -Random stratified sampling    \n",
    "    - Try different algorithms like decision tree,random forest which work well on imbalanced datasets.  \n",
    "    - Penalized models , costSensitiveClassifier  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **How to do treat colinearity?**  \n",
    "   - A regression coefficient is not significant even though, theoretically, that variable should be highly correlated with Y.\n",
    "   - When you add or delete an X variable, the regression coefficients change dramatically.\n",
    "   - You see a negative regression coefficient when your response should increase along with X.\n",
    "   - You see a positive regression coefficient when the response should decrease as X increases.\n",
    "   - Your X variables have high pairwise correlations.  \n",
    "   - Change be check through VIF (Variation Inflation Factor)  \n",
    "   - If VIF=1 No multicollinearity,if VIF 5-10 high correlation  \n",
    "   - Remove Highly correlated predictors from the model \n",
    "   - Use principal Component Analysis.  \n",
    "   - Ridge regression / Lasso regression\n",
    "   - Sequential regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **How do you inspect missing data and when are they important?** \n",
    " - Missing data in the training data set can reduce the power / fit of a model or can lead to a biased model because we have not analysed the behavior and relationship with other variables correctly. It can lead to wrong prediction or classification  \n",
    " - Deletion (for simplicity) \n",
    " - Mean/Mode/Median imputation\n",
    " - Similar Case Imputation  \n",
    " - Prediction Model \n",
    " - KNN Imputation (Choice of K-Value is very critical) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Steps of Data Exploration and Preparation** ?  \n",
    " - Variable Indentification : Predictor,Target,data type and category\n",
    " - Univariate Analysis : Each Variable continuos/Categorical ,spread,Count \n",
    " - Bi-Variate Analysis  \n",
    " - Missing Value Treatment  \n",
    " - Outlier Treatment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **What is Feature Engineering**?  \n",
    " - Feature engineering is the science (and art) of extracting more information from existing data. You are not adding any new data here, but you are actually making the data you already have more useful. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- **Variable Transformation**?  \n",
    " - Symmetric distribution is preferred over skewed distribution as it is easier to interpret and generate inferences.  \n",
    " - Some modeling techniques requires normal distribution of variables. So, whenever we have a skewed distribution, we can use transformations which reduce skewness. \n",
    " - For right skewed distribution, we take square / cube root or logarithm of variable and for left skewed, we take square / cube or exponential of variables \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - **How does a neural network with one layer and one input and output compare to logistics regression?**\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **For a long sorted list and a short ( 4 element list),what algorithm would you use to search the list for the 4 elements ?** \n",
    " - Binary Search \n",
    " - Hash table O(n) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Given an unfair coin with the probability of heads not equal to 0.5.What algorithm could you use to create a list of randon 1s and 0s?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ** Comparing Lasso and Ridge regression** ?  \n",
    " - Also, keep in mind that normalizing the inputs is generally a good idea in every type of regression and should be used in case of ridge regression as well.  \n",
    " - Ridge and Lasso regression work by penalizing the magnitude of coefficients of features along with minimizing the error between predicted and actual observations. These are called ‘regularization’ techniques. The key difference is in how they assign penalty to the coefficients  \n",
    " - Ridge Regression: \n",
    "   - Performs L2 regularization i,e adss penalty equivalent to square of the magnitude of coefficients  \n",
    "   - Minimization objective = LS Onj + alpha*(sum of square of coefficeints)  \n",
    " - Lasso Regression:\n",
    "   - Performs L1 regularization i,e adds penalty equivalent to absolute value of the magnitude of coefficeints  \n",
    "   - Minimization of objective = LS Obj + alpha*(sum of absolute value of coefficients) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Describe SVM**  \n",
    "Done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **What's the difference between MLE and MAP inference?**  \n",
    " - ML is intuitive/naieve in that it starts with the probability of observation given the parameter and tries to find the parameter best according with the observation BUt it take into no consideration the prior knowledge.  \n",
    " \n",
    " - MAP seems more reasonable because it does take into consideration the prior knowledge through the Bayes rules. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **What is boosting**  ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Given a number, how to find a closest number in a series of floating point data ** ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " - ** Discriminative vs Generative**  \n",
    "     Discriminative : \"Ex logistic regression,SVM,Decision Tree Models directly from  p(c|x)    \n",
    "     - What differenciate class A from class B    \n",
    "     - Dont try to model all features instead focus on task of categorizing    \n",
    "\n",
    "     Generative : \"Ex naieve bayes , models P(c|x) = p(x|c)p(c)    \n",
    "     - Can create complete input feature vector   \n",
    "     - Bayes net    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
